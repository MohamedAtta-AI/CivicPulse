{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5bdccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.7 environment at: C:\\Users\\mwmma\\miniconda3\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 4.92s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 1.97s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 44ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpraw\u001b[0m\u001b[2m==7.8.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprawcore\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mupdate-checker\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsocket-client\u001b[0m\u001b[2m==1.9.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install praw --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24bd052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_SUBREDDITS = [\n",
    "    \"Netherlands\",\n",
    "    \"thenetherlands\",\n",
    "    \"dutch\",\n",
    "    \"Nederland\",\n",
    "    \"Netherlands_Memes\",\n",
    "    \"NetherlandsHousing\",\n",
    "    \"StudyInTheNetherlands\",\n",
    "    \"TheHague\",\n",
    "    \"Amsterdam\",\n",
    "    \"Rentbusters\",\n",
    "    \"europe\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_link(name:str) -> str:\n",
    "    return f\"https://www.reddit.com/r/{name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84643560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/thenetherlands/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subreddit_link(RELEVANT_SUBREDDITS[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fe5deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = RELEVANT_SUBREDDITS[6]\n",
    "KEYWORD = \"De Broodfabriek\"\n",
    "HEADERS = {\"User-Agent\": \"scraper-(yourname)-v1.0\"}\n",
    "OUTPUT_POSTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_posts.csv\"\n",
    "OUTPUT_COMMENTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_comments.csv\"\n",
    "\n",
    "RATE_LIMIT = 100          # Reddit cap ‚âà100 req/min\n",
    "DELAY = 60 / RATE_LIMIT   # seconds between requests\n",
    "BACKOFF_TIME = 10         # pause if 429 returned\n",
    "\n",
    "\n",
    "def safe_request(url, params=None):\n",
    "    \"\"\"GET with automatic rate-limit backoff.\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(url, headers=HEADERS, params=params)\n",
    "        if r.status_code == 429:\n",
    "            tqdm.write(\"‚è≥ Rate limited ‚Äî backing off for 10 s...\")\n",
    "            time.sleep(BACKOFF_TIME)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        time.sleep(DELAY)  # respect per-request delay\n",
    "        return r\n",
    "\n",
    "\n",
    "def fetch_posts(after=None):\n",
    "    \"\"\"Fetch subreddit search results (100 per page).\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/search.json\"\n",
    "    params = {\n",
    "        \"q\": KEYWORD,\n",
    "        \"restrict_sr\": \"on\",\n",
    "        \"sort\": \"new\",\n",
    "        \"after\": after,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    return safe_request(url, params).json()\n",
    "\n",
    "\n",
    "def fetch_comments(post_id):\n",
    "    \"\"\"Fetch comments for a given post.\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/comments/{post_id}.json\"\n",
    "    return safe_request(url, {\"limit\": 500}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "387243a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for posts mentioning 'De Broodfabriek' in r/StudyInTheNetherlands ‚Ä¶\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching posts:   0%|                                                        | 0/50 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found 0 posts mentioning De Broodfabriek.\n",
      "\n",
      "üíæ Posts saved to: datasets/StudyInTheNetherlands_De Broodfabriek_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching comments: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Comments saved to: datasets/StudyInTheNetherlands_De Broodfabriek_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "after = None\n",
    "all_posts = []\n",
    "\n",
    "print(f\"üîç Searching for posts mentioning '{KEYWORD}' in r/{SUBREDDIT} ‚Ä¶\\n\")\n",
    "\n",
    "for _ in tqdm(range(50), desc=\"Fetching posts\", ncols=100):\n",
    "    data = fetch_posts(after)\n",
    "    posts = data[\"data\"][\"children\"]\n",
    "    if not posts:\n",
    "        break\n",
    "    all_posts.extend(posts)\n",
    "    after = data[\"data\"].get(\"after\")\n",
    "    if not after:\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(all_posts)} posts mentioning {KEYWORD}.\\n\")\n",
    "\n",
    "# --- Save posts ---\n",
    "with open(OUTPUT_POSTS, \"w\", newline=\"\", encoding=\"utf-8\") as f_posts:\n",
    "    writer = csv.writer(f_posts)\n",
    "    writer.writerow([\"post_id\", \"title\", \"author\", \"created_utc\", \"score\",\n",
    "                     \"num_comments\", \"url\"])\n",
    "    for p in all_posts:\n",
    "        d = p[\"data\"]\n",
    "        writer.writerow([\n",
    "            d[\"id\"], d[\"title\"], d.get(\"author\"),\n",
    "            d[\"created_utc\"], d[\"score\"], d[\"num_comments\"], d[\"url\"]\n",
    "        ])\n",
    "print(\"üíæ Posts saved to:\", OUTPUT_POSTS)\n",
    "\n",
    "# --- Fetch comments ---\n",
    "with open(OUTPUT_COMMENTS, \"w\", newline=\"\", encoding=\"utf-8\") as f_comments:\n",
    "    writer = csv.writer(f_comments)\n",
    "    writer.writerow([\"post_id\", \"comment_id\", \"author\", \"created_utc\",\n",
    "                     \"body\", \"score\", \"parent_id\"])\n",
    "\n",
    "    for p in tqdm(all_posts, desc=\"Fetching comments\", ncols=100):\n",
    "        post_id = p[\"data\"][\"id\"]\n",
    "        try:\n",
    "            comments_json = fetch_comments(post_id)\n",
    "            for c in comments_json[1][\"data\"][\"children\"]:\n",
    "                if c[\"kind\"] != \"t1\":\n",
    "                    continue\n",
    "                cd = c[\"data\"]\n",
    "                writer.writerow([\n",
    "                    post_id, cd[\"id\"], cd.get(\"author\"), cd[\"created_utc\"],\n",
    "                    cd[\"body\"].replace(\"\\n\", \" \"),\n",
    "                    cd[\"score\"], cd[\"parent_id\"]\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"‚ö†Ô∏è Error fetching comments for {post_id}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Done! Comments saved to:\", OUTPUT_COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3537872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a715733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Amsterdam_rijswijk_posts.csv (2 rows)\n",
      "‚úÖ Loaded dutch_rijswijk_posts.csv (1 rows)\n",
      "‚úÖ Loaded europe_rijswijk_posts.csv (1 rows)\n",
      "‚úÖ Loaded Nederland_rijswijk_posts.csv (1 rows)\n",
      "‚úÖ Loaded NetherlandsHousing_rijswijk_posts.csv (8 rows)\n",
      "‚úÖ Loaded Netherlands_Memes_rijswijk_posts.csv (0 rows)\n",
      "‚úÖ Loaded Netherlands_rijswijk_posts.csv (22 rows)\n",
      "‚úÖ Loaded Rentbusters_rijswijk_posts.csv (2 rows)\n",
      "‚úÖ Loaded StudyInTheNetherlands_rijswijk_posts.csv (2 rows)\n",
      "‚úÖ Loaded TheHague_rijswijk_posts.csv (81 rows)\n",
      "‚úÖ Loaded thenetherlands_rijswijk_posts.csv (21 rows)\n",
      "‚úÖ Loaded Amsterdam_rijswijk_comments.csv (43 rows)\n",
      "‚úÖ Loaded dutch_rijswijk_comments.csv (1 rows)\n",
      "‚úÖ Loaded europe_rijswijk_comments.csv (0 rows)\n",
      "‚úÖ Loaded Nederland_rijswijk_comments.csv (5 rows)\n",
      "‚úÖ Loaded NetherlandsHousing_rijswijk_comments.csv (33 rows)\n",
      "‚úÖ Loaded Netherlands_Memes_rijswijk_comments.csv (0 rows)\n",
      "‚úÖ Loaded Netherlands_rijswijk_comments.csv (472 rows)\n",
      "‚úÖ Loaded Rentbusters_rijswijk_comments.csv (2 rows)\n",
      "‚úÖ Loaded StudyInTheNetherlands_rijswijk_comments.csv (8 rows)\n",
      "‚úÖ Loaded TheHague_rijswijk_comments.csv (510 rows)\n",
      "‚úÖ Loaded thenetherlands_rijswijk_comments.csv (258 rows)\n",
      "\n",
      "üìä Total combined rows: 1473\n",
      "üíæ Combined CSV saved as 'combined_rijswijk_data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mwmma\\AppData\\Local\\Temp\\ipykernel_7664\\1259660943.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- Find all matching CSV files ---\n",
    "post_files = glob.glob(\"*_rijswijk_posts.csv\")\n",
    "comment_files = glob.glob(\"*_rijswijk_comments.csv\")\n",
    "all_files = post_files + comment_files\n",
    "\n",
    "if not all_files:\n",
    "    print(\"‚ùå No matching CSV files found.\")\n",
    "    exit()\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        # Handle malformed rows and quoted text safely\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            encoding=\"utf-8\",\n",
    "            on_bad_lines=\"skip\",      # skip problematic rows\n",
    "            quoting=1,                # QUOTE_ALL\n",
    "            engine=\"python\",          # more tolerant\n",
    "            sep=\",\"\n",
    "        )\n",
    "        df[\"source_file\"] = os.path.basename(file_path)\n",
    "        dfs.append(df)\n",
    "        print(f\"‚úÖ Loaded {file_path} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {file_path}: {e}\")\n",
    "\n",
    "# --- Combine all ---\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nüìä Total combined rows: {len(combined_df)}\")\n",
    "\n",
    "# --- Save final combined CSV ---\n",
    "combined_df.to_csv(\"combined_rijswijk_data.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Combined CSV saved as 'combined_rijswijk_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57749e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned data saved to: cleaned_rijswijk_data.csv\n",
      "üìä Total rows: 1460\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combined_df = pd.read_csv(\n",
    "    \"combined_rijswijk_data.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    on_bad_lines=\"skip\",      # skip problematic rows\n",
    "    quoting=1,                # QUOTE_ALL\n",
    "    engine=\"python\",          # more tolerant\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create merged 'content' column\n",
    "combined_df[\"content\"] = (\n",
    "    combined_df[\"title\"].fillna(\"\") + \" \" + combined_df[\"body\"].fillna(\"\")\n",
    ").str.strip()\n",
    "\n",
    "# Add content_type based on filename pattern in source_file\n",
    "combined_df[\"content_type\"] = np.where(\n",
    "    combined_df[\"source_file\"].str.contains(\"_posts\", case=False, na=False),\n",
    "    \"post\",\n",
    "    np.where(\n",
    "        combined_df[\"source_file\"].str.contains(\"_comments\", case=False, na=False),\n",
    "        \"comment\",\n",
    "        \"unknown\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create renamed/filtered final DataFrame\n",
    "cleaned_df = combined_df.rename(columns={\"created_utc\": \"created_at\", \"source_file\": \"source\"})[\n",
    "    [\"post_id\", \"created_at\", \"content\", \"content_type\", \"source\", \"url\", \"score\"]\n",
    "]\n",
    "cleaned_df[\"source\"] = \"reddit\"\n",
    "# --- Remove empty or duplicate content rows ---\n",
    "cleaned_df = cleaned_df[cleaned_df[\"content\"].str.strip().astype(bool)]\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=[\"post_id\", \"content\"])\n",
    "\n",
    "# --- Save to CSV ---\n",
    "output_path = \"cleaned_rijswijk_data.csv\"\n",
    "cleaned_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned data saved to: {output_path}\")\n",
    "print(f\"üìä Total rows: {len(cleaned_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15191dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = RELEVANT_SUBREDDITS[6]\n",
    "KEYWORD = \"De Broodfabriek\"\n",
    "HEADERS = {\"User-Agent\": \"scraper-(yourname)-v1.0\"}\n",
    "OUTPUT_POSTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_posts.csv\"\n",
    "OUTPUT_COMMENTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_comments.csv\"\n",
    "\n",
    "RATE_LIMIT = 100          # Reddit cap ‚âà100 req/min\n",
    "DELAY = 60 / RATE_LIMIT   # seconds between requests\n",
    "BACKOFF_TIME = 10         # pause if 429 returned\n",
    "\n",
    "\n",
    "def safe_request(url, params=None):\n",
    "    \"\"\"GET with automatic rate-limit backoff.\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(url, headers=HEADERS, params=params)\n",
    "        if r.status_code == 429:\n",
    "            tqdm.write(\"‚è≥ Rate limited ‚Äî backing off for 10 s...\")\n",
    "            time.sleep(BACKOFF_TIME)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        time.sleep(DELAY)  # respect per-request delay\n",
    "        return r\n",
    "\n",
    "\n",
    "def fetch_posts(after=None):\n",
    "    \"\"\"Fetch subreddit search results (100 per page).\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/search.json\"\n",
    "    params = {\n",
    "        \"q\": KEYWORD,\n",
    "        \"restrict_sr\": \"on\",\n",
    "        \"sort\": \"new\",\n",
    "        \"after\": after,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    return safe_request(url, params).json()\n",
    "\n",
    "\n",
    "def fetch_comments(post_id):\n",
    "    \"\"\"Fetch comments for a given post.\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/comments/{post_id}.json\"\n",
    "    return safe_request(url, {\"limit\": 500}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = RELEVANT_SUBREDDITS[6]\n",
    "KEYWORD = \"De Broodfabriek\"\n",
    "HEADERS = {\"User-Agent\": \"scraper-(yourname)-v1.0\"}\n",
    "OUTPUT_POSTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_posts.csv\"\n",
    "OUTPUT_COMMENTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_comments.csv\"\n",
    "\n",
    "RATE_LIMIT = 100          # Reddit cap ‚âà100 req/min\n",
    "DELAY = 60 / RATE_LIMIT   # seconds between requests\n",
    "BACKOFF_TIME = 10         # pause if 429 returned\n",
    "\n",
    "\n",
    "def safe_request(url, params=None):\n",
    "    \"\"\"GET with automatic rate-limit backoff.\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(url, headers=HEADERS, params=params)\n",
    "        if r.status_code == 429:\n",
    "            tqdm.write(\"‚è≥ Rate limited ‚Äî backing off for 10 s...\")\n",
    "            time.sleep(BACKOFF_TIME)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        time.sleep(DELAY)  # respect per-request delay\n",
    "        return r\n",
    "\n",
    "\n",
    "def fetch_posts(after=None):\n",
    "    \"\"\"Fetch subreddit search results (100 per page).\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/search.json\"\n",
    "    params = {\n",
    "        \"q\": KEYWORD,\n",
    "        \"restrict_sr\": \"on\",\n",
    "        \"sort\": \"new\",\n",
    "        \"after\": after,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    return safe_request(url, params).json()\n",
    "\n",
    "\n",
    "def fetch_comments(post_id):\n",
    "    \"\"\"Fetch comments for a given post.\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/comments/{post_id}.json\"\n",
    "    return safe_request(url, {\"limit\": 500}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = RELEVANT_SUBREDDITS[6]\n",
    "KEYWORD = \"De Broodfabriek\"\n",
    "HEADERS = {\"User-Agent\": \"scraper-(yourname)-v1.0\"}\n",
    "OUTPUT_POSTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_posts.csv\"\n",
    "OUTPUT_COMMENTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_comments.csv\"\n",
    "\n",
    "RATE_LIMIT = 100          # Reddit cap ‚âà100 req/min\n",
    "DELAY = 60 / RATE_LIMIT   # seconds between requests\n",
    "BACKOFF_TIME = 10         # pause if 429 returned\n",
    "\n",
    "\n",
    "def safe_request(url, params=None):\n",
    "    \"\"\"GET with automatic rate-limit backoff.\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(url, headers=HEADERS, params=params)\n",
    "        if r.status_code == 429:\n",
    "            tqdm.write(\"‚è≥ Rate limited ‚Äî backing off for 10 s...\")\n",
    "            time.sleep(BACKOFF_TIME)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        time.sleep(DELAY)  # respect per-request delay\n",
    "        return r\n",
    "\n",
    "\n",
    "def fetch_posts(after=None):\n",
    "    \"\"\"Fetch subreddit search results (100 per page).\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/search.json\"\n",
    "    params = {\n",
    "        \"q\": KEYWORD,\n",
    "        \"restrict_sr\": \"on\",\n",
    "        \"sort\": \"new\",\n",
    "        \"after\": after,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    return safe_request(url, params).json()\n",
    "\n",
    "\n",
    "def fetch_comments(post_id):\n",
    "    \"\"\"Fetch comments for a given post.\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/comments/{post_id}.json\"\n",
    "    return safe_request(url, {\"limit\": 500}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = RELEVANT_SUBREDDITS[6]\n",
    "KEYWORD = \"De Broodfabriek\"\n",
    "HEADERS = {\"User-Agent\": \"scraper-(yourname)-v1.0\"}\n",
    "OUTPUT_POSTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_posts.csv\"\n",
    "OUTPUT_COMMENTS = f\"datasets/{SUBREDDIT}_{KEYWORD}_comments.csv\"\n",
    "\n",
    "RATE_LIMIT = 100          # Reddit cap ‚âà100 req/min\n",
    "DELAY = 60 / RATE_LIMIT   # seconds between requests\n",
    "BACKOFF_TIME = 10         # pause if 429 returned\n",
    "\n",
    "\n",
    "def safe_request(url, params=None):\n",
    "    \"\"\"GET with automatic rate-limit backoff.\"\"\"\n",
    "    while True:\n",
    "        r = requests.get(url, headers=HEADERS, params=params)\n",
    "        if r.status_code == 429:\n",
    "            tqdm.write(\"‚è≥ Rate limited ‚Äî backing off for 10 s...\")\n",
    "            time.sleep(BACKOFF_TIME)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        time.sleep(DELAY)  # respect per-request delay\n",
    "        return r\n",
    "\n",
    "\n",
    "def fetch_posts(after=None):\n",
    "    \"\"\"Fetch subreddit search results (100 per page).\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/search.json\"\n",
    "    params = {\n",
    "        \"q\": KEYWORD,\n",
    "        \"restrict_sr\": \"on\",\n",
    "        \"sort\": \"new\",\n",
    "        \"after\": after,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    return safe_request(url, params).json()\n",
    "\n",
    "\n",
    "def fetch_comments(post_id):\n",
    "    \"\"\"Fetch comments for a given post.\"\"\"\n",
    "    url = f\"https://www.reddit.com/r/{SUBREDDIT}/comments/{post_id}.json\"\n",
    "    return safe_request(url, {\"limit\": 500}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58394326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleted Amsterdam_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted dutch_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted europe_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted Nederland_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted NetherlandsHousing_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted Netherlands_Memes_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted Netherlands_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted Rentbusters_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted StudyInTheNetherlands_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted TheHague_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted thenetherlands_rijswijk_posts.csv\n",
      "üóëÔ∏è Deleted Amsterdam_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted dutch_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted europe_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted Nederland_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted NetherlandsHousing_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted Netherlands_Memes_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted Netherlands_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted Rentbusters_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted StudyInTheNetherlands_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted TheHague_rijswijk_comments.csv\n",
      "üóëÔ∏è Deleted thenetherlands_rijswijk_comments.csv\n",
      "\n",
      "‚úÖ Cleanup complete! Only 'combined_rijswijk_data.csv' remains.\n"
     ]
    }
   ],
   "source": [
    "# --- Delete original individual CSVs ---\n",
    "for file_path in all_files:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"üóëÔ∏è Deleted {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not delete {file_path}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete! Only 'combined_rijswijk_data.csv' remains.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
